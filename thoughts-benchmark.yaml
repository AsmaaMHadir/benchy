plan_name: "thoughts-benchmark"

pattern: listv1

mode: "architect" # "architect" or "coder"

main_model: "deepseek/deepseek-reasoner"

editor_model: "deepseek/deepseek-chat" # used only if mode is "architect"

editable_context:
  - "src/App.vue"
  - "src/pages/ThoughtBench.vue"
  - "src/stores/thoughtBenchStore.ts"
  - "src/types.d.ts"
  - "src/components/thought_bench/ThoughtColumn.vue"
  
  - "server/utils.py"
  - "server/server.py"
  - "server/modules/ollama_llm.py"
  - "server/modules/data_types.py"
  - "server/modules/llm_models.py"
  - "server/modules/deepseek_llm.py"
  - "server/tests/ollama_llm_test.py"
  - "server/tests/deepseek_llm_test.py"
  - "server/tests/utils_test.py"

readonly_context:
  - "README.md"
  - "src/stores/toolCallStore.ts"
  - "src/stores/isoSpeedBenchStore.ts"
  - "src/pages/IsoSpeedBench.vue"

high_level_objective: "Add a new thoughts benchmark where we can see model thoughts, reasoning and answers"

implementation_details: |
    We'll create a new benchmark called "Thought Bench".

    Add it to App.vue as a new route mirror existing benchmarks.

    Types will go in src/types.d.ts and server/modules/data_types.py for frontend and backend respectively
      we shouldn't have to create any but if you do place them here.

    We'll create the feature in the respective 
      src/store/thoughtBenchStore.ts, src/pages/ThoughtBench.vue, src/components/thought_bench/* files, src/apis/thoughtBenchApi.ts

    In the thoughtBenchStore.ts mirror src/stores/toolCallStore.ts and src/stores/isoSpeedBenchStore.ts for setup
      we'll need to keep track of the models we're testing via ThoughtBenchColumnData
      we'll need to keep track of prompt: string, totalExecutions: number
      for our ThoughtBenchColumnData fill up the data with the following objects
        {model: "ollama:deepseek-r1:1.5b", totalCorrect: 0, responses: [], state: "idle"},
        {model: "ollama:deepseek-r1:8b", totalCorrect: 0, responses: [], state: "idle"},
        {model: "ollama:deepseek-r1:14b", totalCorrect: 0, responses: [], state: "idle"}
        {model: "ollama:deepseek-r1:32b", totalCorrect: 0, responses: [], state: "idle"},
        {model: "ollama:deepseek-r1:70b", totalCorrect: 0, responses: [], state: "idle"},
        {model: "deepseek:deepseek-reasoner", totalCorrect: 0, responses: [], state: "idle"}

    src/pages/ThoughtBench.vue will contain a new benchmark that looks like this...
      We're testing a set of reasoning models.
      We want to view models thoughts and answers.
      At the top we'll have the title "Thought Bench" + description of the benchmark mirror iso speed bench "purpose" + description setup and styling
      We'll then have a button for settings that will show and hide a settings panel (again like iso speed bench)
      Then we have a text area for the prompt.
      Just below that we'll have "Prompt" button
      Then we get to the core of the page.
      We show one column for each model (stored in thoughtBenchStore.ts as dataColumns:ThoughtBenchColumnData[])
        Each column will have a "model name - totalCorrect / total", Then a list of responses. Each response will contain "Thoughts" and "Response" as scrollable, fixed sized divs. Top left we'll show title "Thoughts" and "Response" and then we'll have copy button in the top right for each respective div.
      Models will be preselected in the thoughtBenchStore.ts: dataColumns.
      Now when we click "Prompt" we'll call the api (thoughtBenchApi.ts) and get a ThoughtResponse[]
      We'll then update the dataColumns with their new responses under their respective ThoughtBenchColumnData.responses
      We're going to run these in parallel, whenever results come back in live update so we can see the results as they come in.
        so we'll live update the state of the column to "loading" or "success" or "error" based on the response.
        when we kick off the api call we'll set the state to "loading"
        when we get a response we'll set the state to "success" or "error"
        when we get a response we'll update the totalCorrect for the column

    src/apis/thoughtBenchApi.ts will call out to server/server.py
      we'll need to implement a new endpoint in server/server.py that will accept PromptRequest and return ThoughtResponse
      we'll need a new function in llm_models.py thought_prompt(promptRequest: PromptRequest) -> ThoughtResponse
        we're going to run the same model provider prefix parsing logic as we do for the other benchmarks "deepseek:deepseek-reasoner" -> "deepseek-reasoner" + "deepseek". We'll then pass the model into the right *_llm.thought_prompt module function.
      this will read the model and call the appropriate llm function.
      for our llm functiosn we're going to be making calls to two know thought_prompt functions that we'll need to implement
        server/modules/deepseek_llm.py: thought_prompt(promptRequest: PromptRequest) -> ThoughtResponse
          assert that the model is "deepseek-reasoner"
          then parse the 'thoughts' and 'response' from the response.
            here's the response format you can parse from
              ```json
{
  "id": "40f1d905-b523-4a9f-a160-0672c67076ab",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        # This is the response
        "content": "The most likely next word after \"hello my name i\" is **is**, completing the phrase as:  \n**\"Hello my name is\"**  \n\nCommon follow-ups would then include a name (e.g., *\"Hello my name is John\"*).",
        "refusal": null,
        "role": "assistant",
        "audio": null,
        "function_call": null,
        "tool_calls": null,
        # This is the thoughts
        "reasoning_content": "Okay, so the user wants an autocomplete suggestion for the next word after \"hello my name i\". Let me think about this. The phrase starts with \"hello my name i\", and the user is probably trying to say \"Hello, my name is...\". Since the last word they typed is \"i\", which is likely a typo or an incomplete \"is\".\n\nFirst, check if \"i\" is followed by \"s\" to make \"is\". That's the most common structure. So the next word would be \"is\". Then after that, typically a name follows, like \"John\" or \"Alice\". But since the query is specifically asking for the next word, it's probably \"is\".\n\nBut wait, maybe the user typed \"i\" instead of \"is\" by accident. Autocomplete might correct that. Alternatively, sometimes people might start with \"I\" as in \"Hello, my name I...\", but that's less common. In standard English, it should be \"is\". So the most probable next word is \"is\". Let me confirm by thinking of similar phrases. \"Hello, my name is...\" is the standard introduction. If someone types \"i\" instead of \"is\", the system should still suggest \"is\" as the next word. So the answer is \"is\"."
      }
    }
  ],
  "created": 1737815520,
  "model": "deepseek-reasoner",
  "object": "chat.completion",
  "service_tier": null,
  "system_fingerprint": "fp_1c5d8833bc",
  "usage": {
    "completion_tokens": 321,
    "prompt_tokens": 17,
    "total_tokens": 338,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": null,
      "reasoning_tokens": 266,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": null,
      "cached_tokens": 0
    },
    "prompt_cache_hit_tokens": 0,
    "prompt_cache_miss_tokens": 17
  }
}
              ```
        server/modules/ollama_llm.py: thought_prompt(promptRequest: PromptRequest) -> ThoughtResponse
          Now for these models we need to assert that model contains 'deepseek-r1'.
          If not throw an error.
          Then we need to parse the response.
          We'll need to implement deepseek_r1_distil_separate_thoughts_and_response(response: str) -> ThoughtResponse in utils.py
          And return the result from that

    We'll need to implement deepseek_r1_distil_separate_thoughts_and_response(response: str) -> ThoughtResponse in utils.py
      Basically we need to parse "thoughts" and "response" from the response like this
        ```txt
<think>
Okay, so I need to figure out what the capital of France is. Hmm, let me think....
</think>

The capital of France is Paris. Despite historical events such as the German occupation during World War II, when the French government was temporarily relocated elsewhere, Paris has consistently been recognized as the official capital. It holds significance as the cultural, economic, and political heart of France, with institutions like the Élysée Palace and major landmarks reinforcing its status. Therefore, Paris remains the central administrative and symbolic capital of the country.

**Answer:** The capital of France is Paris.
        ```
        
        thoughts will be "Okay, so I need to figure out what the capital of France is. Hmm, let me think...."
        response will be "The capital of France is Paris. Despite historical events such as the German occupation during World War II, when the French government was temporarily relocated elsewhere, Paris has consistently been recognized as the official capital. It holds significance as the cultural, economic, and political heart of France, with institutions like the Élysée Palace and major landmarks reinforcing its status. Therefore, Paris remains the central administrative and symbolic capital of the country.

**Answer:** The capital of France is Paris."

        the thoughts will always be in between <think> and </think>. parse the text between them and return it as the thoughts. then remove the <think> and </think> from the response, trim and return it as the response.

    Now we'll need to implement the tests in server/tests/deepseek_llm_test.py for thought_prompt method in deepseek_llm.py
    Then we'll need to implement the tests in server/tests/ollama_llm_test.py for thought_prompt method in ollama_llm.py
    Then we'll implement tests for utils.py in server/tests/utils_test.py for deepseek_r1_distil_separate_thoughts_and_response method



# your list of tasks aka prompts that will be executed in order one by one
tasks:
  - title: "Add new benchmark to App.vue"
    prompt: |
      UPDATE src/App.vue:
        Add a new route called "thought-bench"
        Add a new card for it after the "iso-speed-bench" card
  - title: "Build out the store"
    prompt: |
      UPDATE src/stores/thoughtBenchStore.ts:
        Add a new store with the following properties:
          dataColumns: ThoughtBenchColumnData[]
          prompt: string
          totalExecutions: number
          apiCallInProgress: boolean
          settings: {
            modelStatDetail: 'verbose' | 'hide' # show / hide the header + description based on this
          }
        Update dataColumns to have the following objects:
          {model: "ollama:deepseek-r1:1.5b", totalCorrect: 0, responses: [], state: "idle"},
          {model: "ollama:deepseek-r1:8b", totalCorrect: 0, responses: [], state: "idle"},
          {model: "ollama:deepseek-r1:14b", totalCorrect: 0, responses: [], state: "idle"}
          {model: "ollama:deepseek-r1:32b", totalCorrect: 0, responses: [], state: "idle"},
          {model: "ollama:deepseek-r1:70b", totalCorrect: 0, responses: [], state: "idle"},
          {model: "deepseek:deepseek-reasoner", totalCorrect: 0, responses: [], state: "idle"}
  - title: "Build out the page"
    prompt: |
      ...
